{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAP - Charting Student Math Misunderstandings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.328805Z",
     "iopub.status.busy": "2025-08-26T01:40:08.328167Z",
     "iopub.status.idle": "2025-08-26T01:40:08.334287Z",
     "shell.execute_reply": "2025-08-26T01:40:08.333333Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.328775Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import torch.optim as optim\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.util import trigrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.340036Z",
     "iopub.status.busy": "2025-08-26T01:40:08.339807Z",
     "iopub.status.idle": "2025-08-26T01:40:08.459682Z",
     "shell.execute_reply": "2025-08-26T01:40:08.459116Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.340020Z"
    }
   },
   "outputs": [],
   "source": [
    "#  Load & train and test data\n",
    "data = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/train.csv')\n",
    "test_df_original = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.460916Z",
     "iopub.status.busy": "2025-08-26T01:40:08.460661Z",
     "iopub.status.idle": "2025-08-26T01:40:08.487495Z",
     "shell.execute_reply": "2025-08-26T01:40:08.486628Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.460889Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.488848Z",
     "iopub.status.busy": "2025-08-26T01:40:08.488363Z",
     "iopub.status.idle": "2025-08-26T01:40:08.495086Z",
     "shell.execute_reply": "2025-08-26T01:40:08.494506Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.488814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Impute missing values for the `Misconception` column with NA\n",
    "data['Misconception'] = data['Misconception'].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.496923Z",
     "iopub.status.busy": "2025-08-26T01:40:08.496678Z",
     "iopub.status.idle": "2025-08-26T01:40:08.775138Z",
     "shell.execute_reply": "2025-08-26T01:40:08.774559Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.496905Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the is_correct feature\n",
    "idx = data.apply(lambda row: row.Category.split('_')[0], axis=1) == 'True'\n",
    "tmp = data.loc[idx].copy()\n",
    "tmp['c'] = tmp.groupby(['QuestionId', 'MC_Answer']).MC_Answer.transform('count')\n",
    "tmp = tmp.sort_values('c', ascending=False)\n",
    "tmp = tmp.drop_duplicates(['QuestionId'])\n",
    "tmp = tmp[['QuestionId', 'MC_Answer']]\n",
    "tmp['is_correct'] = 1\n",
    "\n",
    "# Create for train dataset\n",
    "data = data.merge(tmp, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "data.is_correct = data.is_correct.fillna(0)\n",
    "\n",
    "# Create for test dataset\n",
    "test_df_original = test_df_original.merge(tmp, on=['QuestionId','MC_Answer'], how='left')\n",
    "test_df_original.is_correct = test_df_original.is_correct.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.775985Z",
     "iopub.status.busy": "2025-08-26T01:40:08.775750Z",
     "iopub.status.idle": "2025-08-26T01:40:08.819356Z",
     "shell.execute_reply": "2025-08-26T01:40:08.818831Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.775967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the train_text feature\n",
    "data['train_text'] = (\n",
    "data['QuestionId'].astype(str) + \" \" +\n",
    "data['QuestionText'].astype(str) + \" \" +\n",
    "data['MC_Answer'].astype(str) + \" \" +\n",
    "data['StudentExplanation'].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.820393Z",
     "iopub.status.busy": "2025-08-26T01:40:08.820164Z",
     "iopub.status.idle": "2025-08-26T01:40:08.826829Z",
     "shell.execute_reply": "2025-08-26T01:40:08.826278Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.820375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to /usr/share/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /usr/share/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.util import trigrams\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger_eng') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:08.829046Z",
     "iopub.status.busy": "2025-08-26T01:40:08.828579Z",
     "iopub.status.idle": "2025-08-26T01:40:30.251091Z",
     "shell.execute_reply": "2025-08-26T01:40:30.250498Z",
     "shell.execute_reply.started": "2025-08-26T01:40:08.829028Z"
    }
   },
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "def preprocess_text(row):\n",
    "    # Extract and preprocess each component\n",
    "    question_id = str(row['QuestionId']).strip()\n",
    "    question_text = str(row['QuestionText']).lower().strip()\n",
    "    mc_answer = str(row['MC_Answer']).lower().strip()\n",
    "    student_explanation = str(row['StudentExplanation']).lower().strip()\n",
    "\n",
    "    # Convert frac notation to proper fraction\n",
    "    question_text = re.sub(r'frac(\\d+)/(\\d+)', r'\\1/\\2', question_text)\n",
    "    mc_answer = re.sub(r'frac(\\d+)/(\\d+)', r'\\1/\\2', mc_answer)\n",
    "    student_explanation = re.sub(r'frac(\\d+)/(\\d+)', r'\\1/\\2', student_explanation)\n",
    "\n",
    "    # Remove excessive noise, preserve numbers and fractions\n",
    "    question_text = re.sub(r'[^a-z0-9\\s/]', '', question_text)\n",
    "    mc_answer = re.sub(r'[^a-z0-9\\s/]', '', mc_answer)\n",
    "    student_explanation = re.sub(r'[^a-z0-9\\s/]', '', student_explanation)\n",
    "\n",
    "    # Tokenize and lemmatize (optional for explanation, skip for IDs and answers to preserve exact terms)\n",
    "    tokens = word_tokenize(student_explanation)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatized_explanation = ' '.join([lemmatizer.lemmatize(token, get_wordnet_pos(tag)) if not token.isdigit() and '/' not in token else token for token, tag in tagged_tokens])\n",
    "\n",
    "    # Construct structured train_text\n",
    "    train_text = f\"[QID] {question_id} [QTXT] {question_text} [ANS] {mc_answer} [EXP] {lemmatized_explanation}\"\n",
    "    return train_text.strip()\n",
    "\n",
    "# Apply preprocessing to train_text feature\n",
    "data['train_text'] = data.apply(preprocess_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.252158Z",
     "iopub.status.busy": "2025-08-26T01:40:30.251875Z",
     "iopub.status.idle": "2025-08-26T01:40:30.262733Z",
     "shell.execute_reply": "2025-08-26T01:40:30.262000Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.252131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>QuestionId</th>\n",
       "      <th>QuestionText</th>\n",
       "      <th>MC_Answer</th>\n",
       "      <th>StudentExplanation</th>\n",
       "      <th>Category</th>\n",
       "      <th>Misconception</th>\n",
       "      <th>is_correct</th>\n",
       "      <th>train_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>0ne third is equal to tree nineth</td>\n",
       "      <td>True_Correct</td>\n",
       "      <td>NA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[QID] 31772 [QTXT] what fraction of the shape ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>31772</td>\n",
       "      <td>What fraction of the shape is not shaded? Give...</td>\n",
       "      <td>\\( \\frac{1}{3} \\)</td>\n",
       "      <td>1 / 3 because 6 over 9 is 2 thirds and 1 third...</td>\n",
       "      <td>True_Correct</td>\n",
       "      <td>NA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[QID] 31772 [QTXT] what fraction of the shape ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   row_id  QuestionId                                       QuestionText  \\\n",
       "0       0       31772  What fraction of the shape is not shaded? Give...   \n",
       "1       1       31772  What fraction of the shape is not shaded? Give...   \n",
       "\n",
       "           MC_Answer                                 StudentExplanation  \\\n",
       "0  \\( \\frac{1}{3} \\)                  0ne third is equal to tree nineth   \n",
       "1  \\( \\frac{1}{3} \\)  1 / 3 because 6 over 9 is 2 thirds and 1 third...   \n",
       "\n",
       "       Category Misconception  is_correct  \\\n",
       "0  True_Correct            NA         1.0   \n",
       "1  True_Correct            NA         1.0   \n",
       "\n",
       "                                          train_text  \n",
       "0  [QID] 31772 [QTXT] what fraction of the shape ...  \n",
       "1  [QID] 31772 [QTXT] what fraction of the shape ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.263935Z",
     "iopub.status.busy": "2025-08-26T01:40:30.263386Z",
     "iopub.status.idle": "2025-08-26T01:40:30.272399Z",
     "shell.execute_reply": "2025-08-26T01:40:30.271884Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.263916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[QID] 31772 [QTXT] what fraction of the shape is not shaded give your answer in its simplest form image a triangle split into 9 equal smaller triangles 6 of them are shaded [ANS]  frac13  [EXP] 1/3 because i work out that 3/9 be not shade and because 3 and 9 have a common factor of 3 i divide the numerator and denominator by 3 to give me 1/3'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[8, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.273250Z",
     "iopub.status.busy": "2025-08-26T01:40:30.273047Z",
     "iopub.status.idle": "2025-08-26T01:40:30.320015Z",
     "shell.execute_reply": "2025-08-26T01:40:30.319491Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.273227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the combined_label feature (multi-class target)\n",
    "data['combined_label'] = data['Category'].astype(str) + ':' + data['Misconception'].astype(str)\n",
    "\n",
    "# Drop labels that have less than 2 instances\n",
    "vc = data['combined_label'].value_counts()\n",
    "classes_to_keep = vc[vc >= 2].index\n",
    "train_filtered = data[data['combined_label'].isin(classes_to_keep)].copy()\n",
    "\n",
    "# Encode the combined_label to numeric values\n",
    "label_le = LabelEncoder()\n",
    "train_filtered['combined_label_encoded'] = label_le.fit_transform(train_filtered['combined_label'])\n",
    "y_encoded = train_filtered['combined_label_encoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.320857Z",
     "iopub.status.busy": "2025-08-26T01:40:30.320692Z",
     "iopub.status.idle": "2025-08-26T01:40:30.331042Z",
     "shell.execute_reply": "2025-08-26T01:40:30.330380Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.320843Z"
    }
   },
   "outputs": [],
   "source": [
    "# Encode QuestionId with a stable mapping (0 reserved for UNK/PAD)\n",
    "unique_question_ids = sorted(train_filtered['QuestionId'].unique())\n",
    "question_id_to_encoded = {q_id: i + 1 for i, q_id in enumerate(unique_question_ids)}\n",
    "question_id_to_encoded['UNSEEN_ID'] = 0 # reserve 0 for unknown/pad\n",
    "train_filtered['QuestionId_encoded'] = train_filtered['QuestionId'].map(question_id_to_encoded).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.332284Z",
     "iopub.status.busy": "2025-08-26T01:40:30.331937Z",
     "iopub.status.idle": "2025-08-26T01:40:30.339395Z",
     "shell.execute_reply": "2025-08-26T01:40:30.338716Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.332260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of misconception classes: 60\n",
      "Number of unique Question IDs: 15\n"
     ]
    }
   ],
   "source": [
    "# Save label mappings \n",
    "joblib.dump(dict(enumerate(label_le.classes_)), 'label_to_original.pkl')\n",
    "joblib.dump(question_id_to_encoded, 'question_id_to_encoded.pkl')\n",
    "print(f\"Number of misconception classes: {len(label_le.classes_)}\")\n",
    "print(f\"Number of unique Question IDs: {len(unique_question_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.340705Z",
     "iopub.status.busy": "2025-08-26T01:40:30.340147Z",
     "iopub.status.idle": "2025-08-26T01:40:30.368008Z",
     "shell.execute_reply": "2025-08-26T01:40:30.367292Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.340681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform Train-Test split\n",
    "train_features_df = train_filtered[['train_text', 'QuestionId_encoded', 'is_correct', 'combined_label_encoded']]\n",
    "train_df, val_df = train_test_split(\n",
    "train_features_df,\n",
    "test_size=0.2,\n",
    "random_state=42,\n",
    "stratify=y_encoded\n",
    ")\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "# define num_classes (indices go from 0..U where 0 is UNK/PAD and max is len(unique_question_ids))\n",
    "num_classes = len(label_le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.368921Z",
     "iopub.status.busy": "2025-08-26T01:40:30.368735Z",
     "iopub.status.idle": "2025-08-26T01:40:30.375423Z",
     "shell.execute_reply": "2025-08-26T01:40:30.374817Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.368907Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define dataset classes\n",
    "num_question_ids = len(unique_question_ids) + 1  \n",
    "\n",
    "class MathMisconceptionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        \"\"\"\n",
    "        Dataset for Math Misconception task, compatible with SciBERT tokenizer\n",
    "        (allenai/scibert_scivocab_uncased).\n",
    "        \"\"\"\n",
    "        self.texts = dataframe['train_text'].values\n",
    "        self.question_ids = dataframe['QuestionId_encoded'].values\n",
    "        self.is_corrects = dataframe['is_correct'].values\n",
    "        self.labels = dataframe['combined_label_encoded'].values\n",
    "        self.tokenizer = tokenizer  # Expected to be SciBERT tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        question_id = int(self.question_ids[idx])\n",
    "        is_correct = float(self.is_corrects[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'question_id': torch.tensor(question_id, dtype=torch.long),\n",
    "            'is_correct': torch.tensor(is_correct, dtype=torch.float),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.378381Z",
     "iopub.status.busy": "2025-08-26T01:40:30.378159Z",
     "iopub.status.idle": "2025-08-26T01:40:30.391710Z",
     "shell.execute_reply": "2025-08-26T01:40:30.391023Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.378352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define custom SciBERT model\n",
    "class CustomSciBERTModel(nn.Module):\n",
    "    def __init__(self, scibert_model, num_classes, num_question_ids, question_id_embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.scibert = scibert_model\n",
    "        self.question_id_embedding = nn.Embedding(num_question_ids, question_id_embedding_dim, padding_idx=0)\n",
    "        self.is_correct_layer = nn.Linear(1, 16)\n",
    "        hidden = self.scibert.config.hidden_size + question_id_embedding_dim + 16\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.scibert.config.hidden_dropout_prob),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, question_id, is_correct):\n",
    "        scibert_out = self.scibert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        qid_emb = self.question_id_embedding(question_id)\n",
    "        ic_emb = self.is_correct_layer(is_correct.unsqueeze(1).float()) # ensure float for Linear\n",
    "        features = torch.cat((scibert_out, qid_emb, ic_emb), dim=1)\n",
    "        logits = self.classifier(features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.392752Z",
     "iopub.status.busy": "2025-08-26T01:40:30.392505Z",
     "iopub.status.idle": "2025-08-26T01:40:30.406162Z",
     "shell.execute_reply": "2025-08-26T01:40:30.405485Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.392730Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to compute MAP@3 metric\n",
    "def map_at_3(predictions, true_labels):\n",
    "    top_3_pred = np.argsort(predictions, axis=1)[:, -3:][:, ::-1]\n",
    "    aps = []\n",
    "    for i in range(len(true_labels)):\n",
    "        true_label = true_labels[i]\n",
    "        preds = top_3_pred[i]\n",
    "        hits = (preds == true_label)\n",
    "        if not np.any(hits):\n",
    "            aps.append(0.0)\n",
    "        else:\n",
    "            rank = np.where(hits)[0][0] + 1\n",
    "            aps.append(1.0 / rank)\n",
    "    return float(np.mean(aps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.407057Z",
     "iopub.status.busy": "2025-08-26T01:40:30.406767Z",
     "iopub.status.idle": "2025-08-26T01:40:30.422852Z",
     "shell.execute_reply": "2025-08-26T01:40:30.422157Z",
     "shell.execute_reply.started": "2025-08-26T01:40:30.407040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train / Eval loops\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc='Training'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        question_id = batch['question_id'].to(device)\n",
    "        is_correct = batch['is_correct'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                       question_id=question_id, is_correct=is_correct)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        (loss / accumulation_steps).backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0 or (step + 1) == len(dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    preds_np = np.concatenate(all_preds, axis=0)\n",
    "    labels_np = np.concatenate(all_labels, axis=0)\n",
    "    train_map3 = map_at_3(preds_np, labels_np)\n",
    "    return avg_loss, train_map3\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=len(dataloader), desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            question_id = batch['question_id'].to(device)\n",
    "            is_correct = batch['is_correct'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                           question_id=question_id, is_correct=is_correct)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_preds.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    preds_np = np.concatenate(all_preds, axis=0)\n",
    "    labels_np = np.concatenate(all_labels, axis=0)\n",
    "    val_map3 = map_at_3(preds_np, labels_np)\n",
    "    return avg_loss, val_map3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-26T01:40:30.423962Z",
     "iopub.status.busy": "2025-08-26T01:40:30.423692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Epoch 1/3\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 2281/3669 [12:57<07:51,  2.94it/s]"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizer, Pretrained SciBERT transformer and training\n",
    "\n",
    "# Define hyperparameters\n",
    "MAX_LEN = 256\n",
    "BATCH_SIZE = 8\n",
    "ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "EPOCHS = 3\n",
    "WARMUP_STEPS = 0\n",
    "\n",
    "# Tokenizer & base model\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "config = AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "base_scibert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=config)\n",
    "\n",
    "# Model\n",
    "aodel = CustomSciBERTModel(\n",
    "    scibert_model=base_scibert_model,\n",
    "    num_classes=num_classes,\n",
    "    num_question_ids=num_question_ids,\n",
    ")\n",
    "\n",
    "# Optimizer, device, loaders\n",
    "optimizer = optim.AdamW(aodel.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "aodel.to(device)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dataset = MathMisconceptionDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_dataset = MathMisconceptionDataset(val_df, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Scheduler (step per optimizer step)\n",
    "update_steps_per_epoch = int(np.ceil(len(train_dataloader) / ACCUMULATION_STEPS))\n",
    "total_steps = update_steps_per_epoch * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=WARMUP_STEPS,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "best_val_map3 = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    print('-' * 30)\n",
    "    train_loss, train_map3 = train_epoch(\n",
    "        aodel, train_dataloader, optimizer, scheduler, device,\n",
    "        accumulation_steps=ACCUMULATION_STEPS,\n",
    "    )\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train MAP@3: {train_map3:.4f}\")\n",
    "\n",
    "    val_loss, val_map3 = eval_model(aodel, val_dataloader, device)\n",
    "    print(f\"Validation Loss: {val_loss:.4f} | Validation MAP@3: {val_map3:.4f}\")\n",
    "\n",
    "    if val_map3 > best_val_map3:\n",
    "        best_val_map3 = val_map3\n",
    "        # Save model and tokenizer\n",
    "        torch.save(aodel.state_dict(), './SciBERT_math_misconception_custom.pth')\n",
    "        tokenizer.save_pretrained('./SciBERT_math_misconception_tokenizer')\n",
    "        \n",
    "        # Create zip file containing model and tokenizer\n",
    "        zip_filename = 'scibert-finetuned.zip'\n",
    "        with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            zipf.write('./SciBERT_math_misconception_custom.pth')\n",
    "            for root, _, files in os.walk('./SciBERT_math_misconception_tokenizer'):\n",
    "                for file in files:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, './')\n",
    "                    zipf.write(file_path, arcname)\n",
    "        \n",
    "        print(f\"Saved improved model and tokenizer as {zip_filename} with Validation MAP@3: {val_map3:.4f}\")\n",
    "        \n",
    "        # Prompt automatic download\n",
    "        display(FileLink(zip_filename))\n",
    "\n",
    "print(\"\\nFine-tuning completed.\")\n",
    "print(f\"Best Validation MAP@3 achieved: {best_val_map3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference finetuned model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load mappings\n",
    "label_to_original = joblib.load('label_to_original.pkl')\n",
    "question_id_to_encoded = joblib.load('question_id_to_encoded.pkl')\n",
    "num_classes = len(label_to_original)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# Prepare test dataframe (create is_correct & text, and ENCODE QuestionId)\n",
    "test_df = test_df_original.copy()\n",
    "\n",
    "# try:\n",
    "    test_df = test_df.merge(tmp, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "    test_df['is_correct'] = test_df['is_correct'].fillna(0.0)\n",
    "    print(\"Merged with `tmp` to get is_correct.\")\n",
    "except NameError:\n",
    "    print(\"Warning: `tmp` not found. Setting `is_correct` = 0 for all samples.\")\n",
    "    test_df['is_correct'] = 0.0\n",
    "\n",
    "test_df['text_test'] = (\n",
    "    test_df['QuestionId'].astype(str) + \" \" +\n",
    "    test_df['QuestionText'].astype(str) + ' ' +\n",
    "    test_df['MC_Answer'].astype(str) + ' ' +\n",
    "    test_df['StudentExplanation'].astype(str)\n",
    ")\n",
    "# Apply preprocessing to text_test feature\n",
    "test_df['text_test'] = test_df.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Encode QuestionId for test (unseen -> 0)\n",
    "test_df['QuestionId_encoded'] = test_df['QuestionId'].map(lambda q: question_id_to_encoded.get(q, 0)).astype(int)\n",
    "\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=256):\n",
    "        self.texts = dataframe['text_test'].values\n",
    "        self.question_ids = dataframe['QuestionId_encoded'].values\n",
    "        self.is_corrects = dataframe['is_correct'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        qid = int(self.question_ids[idx])\n",
    "        ic = float(self.is_corrects[idx])\n",
    "        enc = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0),\n",
    "            'question_id': torch.tensor(qid, dtype=torch.long),\n",
    "            'is_correct': torch.tensor(ic, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "inference_dataset = InferenceDataset(test_df, tokenizer)\n",
    "inference_dataloader = DataLoader(inference_dataset, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Reload model for inference with the SAME shapes\n",
    "loaded_base_scibert_model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased', config=AutoConfig.from_pretrained('allenai/scibert_scivocab_uncased'))\n",
    "model_inference = CustomSciBERTModel(\n",
    "    scibert_model=loaded_base_scibert_model,\n",
    "    num_classes=num_classes,\n",
    "    num_question_ids=num_question_ids,\n",
    ")\n",
    "model_inference.load_state_dict(torch.load('./SciBERT_math_misconception_custom.pth', map_location=device))\n",
    "model_inference.to(device)\n",
    "model_inference.eval()\n",
    "print('Fine-tuned SciBERT model loaded successfully for inference.')\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_scibert(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    for batch in tqdm(dataloader, desc='Predicting'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        question_id = batch['question_id'].to(device)\n",
    "        is_correct = batch['is_correct'].to(device)\n",
    "        logits = model(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                       question_id=question_id, is_correct=is_correct)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.detach().cpu().numpy())\n",
    "    return np.concatenate(all_probs, axis=0)\n",
    "\n",
    "pred_proba = predict_with_scibert(model_inference, inference_dataloader, device)\n",
    "\n",
    "# Decode predictions to top-3 labels\n",
    "submission_strings = []\n",
    "for prob in pred_proba:\n",
    "    top_3_idx = np.argsort(prob)[-3:][::-1]\n",
    "    top_3_labels = [label_to_original[i] for i in top_3_idx]\n",
    "    submission_strings.append(' '.join(top_3_labels))\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'Category:Misconception': submission_strings\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Saved submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Set Using Finetuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths \n",
    "FINE_TUNED_DIR = \"/kaggle/input/scibert_math_misconception/transformers/default/1/scibert_math_misconception\"\n",
    "SCIBERT_BASE_DIR = \"/kaggle/input/scibert-base-offline/transformers/default/1/scibert-base-offline\"\n",
    "\n",
    "# Validate paths\n",
    "assert os.path.exists(FINE_TUNED_DIR), f\"Fine-tuned model dir not found: {FINE_TUNED_DIR}\"\n",
    "assert os.path.exists(SCIBERT_BASE_DIR), f\"SciBERT-base dir not found: {SCIBERT_BASE_DIR}\"\n",
    "\n",
    "# Define the model architecture\n",
    "class CustomSciBERTModel(nn.Module):\n",
    "    def __init__(self, scibert_model, num_classes, num_question_ids, question_id_embedding_dim=64):\n",
    "        super(CustomSciBERTModel, self).__init__()\n",
    "        self.scibert = scibert_model\n",
    "        self.question_id_embedding = nn.Embedding(num_question_ids, question_id_embedding_dim, padding_idx=0)\n",
    "        self.is_correct_layer = nn.Linear(1, 16)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.scibert.config.hidden_size + question_id_embedding_dim + 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.scibert.config.hidden_dropout_prob),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, question_id, is_correct):\n",
    "        # SciBERT outputs\n",
    "        scibert_output = self.scibert(input_ids=input_ids, attention_mask=attention_mask).pooler_output\n",
    "        \n",
    "        # Embed question_id\n",
    "        question_id_emb = self.question_id_embedding(question_id)\n",
    "        \n",
    "        # Embed is_correct (scalar)\n",
    "        is_correct_emb = self.is_correct_layer(is_correct.unsqueeze(1))\n",
    "        \n",
    "        # Concatenate all features\n",
    "        combined_features = torch.cat((scibert_output, question_id_emb, is_correct_emb), dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(combined_features)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mappings\n",
    "label_to_original = joblib.load(os.path.join(FINE_TUNED_DIR, 'label_to_original.pkl'))\n",
    "question_id_to_encoded = joblib.load(os.path.join(FINE_TUNED_DIR, 'question_id_to_encoded.pkl'))\n",
    "\n",
    "num_classes = len(label_to_original)\n",
    "num_question_ids = len(question_id_to_encoded)  # includes padding index\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Number of unique Question IDs: {num_question_ids}\")\n",
    "\n",
    "# Load tokenizer from fine-tuned model directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/scibert_math_misconception/transformers/default/1/scibert_math_misconception/tokenizer\")\n",
    "print(\"Tokenizer loaded from fine-tuned SciBERT model directory.\")\n",
    "\n",
    "# Prepare test data\n",
    "test_df_original = pd.read_csv('/kaggle/input/map-charting-student-math-misunderstandings/test.csv')\n",
    "test_df = test_df_original.copy()\n",
    "\n",
    "try:\n",
    "    test_df = test_df.merge(tmp, on=['QuestionId', 'MC_Answer'], how='left')\n",
    "    test_df['is_correct'] = test_df['is_correct'].fillna(0.0)\n",
    "    print(\"Merged with `tmp` to get is_correct.\")\n",
    "except NameError:\n",
    "    print(\"Warning: `tmp` not found. Setting `is_correct` = 0 for all samples.\")\n",
    "    test_df['is_correct'] = 0.0\n",
    "\n",
    "# Create input text\n",
    "test_df['text_test'] = (\n",
    "    test_df['QuestionId'].astype(str) + \" \" +\n",
    "    test_df['QuestionText'].astype(str) + ' ' +\n",
    "    test_df['MC_Answer'].astype(str) + ' ' +\n",
    "    test_df['StudentExplanation'].astype(str)\n",
    ")\n",
    "\n",
    "# Apply preprocessing to text_test feature\n",
    "test_df['text_test'] = test_df.apply(preprocess_text, axis=1)\n",
    "\n",
    "# Encode QuestionId using saved mapping (unseen → 0)\n",
    "test_df['QuestionId_encoded'] = test_df['QuestionId'].map(\n",
    "    lambda q: question_id_to_encoded.get(q, 0)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference Dataset\n",
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=256):\n",
    "        self.texts = dataframe['text_test'].values\n",
    "        self.question_ids = dataframe['QuestionId_encoded'].values\n",
    "        self.is_corrects = dataframe['is_correct'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        qid = int(self.question_ids[idx])\n",
    "        ic = float(self.is_corrects[idx])\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'question_id': torch.tensor(qid, dtype=torch.long),\n",
    "            'is_correct': torch.tensor(ic, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Create DataLoader\n",
    "inference_dataset = InferenceDataset(test_df, tokenizer)\n",
    "inference_dataloader = DataLoader(\n",
    "    inference_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base SciBERT model FROM LOCAL OFFLINE FILES\n",
    "print(\"Loading SciBERT-base from local offline directory...\")\n",
    "base_scibert_model = AutoModel.from_pretrained(SCIBERT_BASE_DIR)\n",
    "print(\"Base SciBERT model loaded successfully (offline).\")\n",
    "\n",
    "# Instantiate and load custom model\n",
    "model_inference = CustomSciBERTModel(\n",
    "    scibert_model=base_scibert_model,\n",
    "    num_classes=num_classes,\n",
    "    num_question_ids=num_question_ids\n",
    ")\n",
    "\n",
    "# Load your fine-tuned weights\n",
    "model_weights_path = os.path.join(FINE_TUNED_DIR, 'model_weights.pth')\n",
    "print(f\"Loading fine-tuned weights from: {model_weights_path}\")\n",
    "\n",
    "model_inference.load_state_dict(torch.load(model_weights_path, map_location=device))\n",
    "model_inference.to(device)\n",
    "model_inference.eval()\n",
    "print('Fine-tuned SciBERT model loaded successfully for inference.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "@torch.no_grad()\n",
    "def predict_with_scibert(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        question_id = batch['question_id'].to(device)\n",
    "        is_correct = batch['is_correct'].to(device)\n",
    "        \n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            question_id=question_id,\n",
    "            is_correct=is_correct\n",
    "        )\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "    return np.concatenate(all_probs, axis=0)\n",
    "\n",
    "# Run inference\n",
    "pred_proba = predict_with_scibert(model_inference, inference_dataloader, device)\n",
    "\n",
    "# Generate submission\n",
    "submission_strings = []\n",
    "for prob in pred_proba:\n",
    "    top_3_idx = np.argsort(prob)[-3:][::-1]\n",
    "    top_3_labels = [label_to_original[i] for i in top_3_idx]\n",
    "    submission_strings.append(\" \".join(top_3_labels))\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'Category:Misconception': submission_strings\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Saved submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12957508,
     "isSourceIdPinned": false,
     "sourceId": 104383,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 430885,
     "modelInstanceId": 413145,
     "sourceId": 528127,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 432259,
     "modelInstanceId": 414492,
     "sourceId": 529886,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 432264,
     "modelInstanceId": 414496,
     "sourceId": 529891,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 435592,
     "modelInstanceId": 417923,
     "sourceId": 540760,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 435712,
     "modelInstanceId": 418051,
     "sourceId": 541597,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 435769,
     "modelInstanceId": 418104,
     "sourceId": 541860,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 435846,
     "modelInstanceId": 418182,
     "sourceId": 542711,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
